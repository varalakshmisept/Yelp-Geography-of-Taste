{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import date\n",
    "import textstat\n",
    "import readability\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AIT722-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocess(t):\n",
    "    t = t.lower()\n",
    "    # Remove punctuations\n",
    "    t = re.sub(r'[^a-zA-Z]',' ', t) \n",
    "    t = [w for w in t.split() if w not in stopwords.words('english')]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: dataPreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "def lemmatize_token(tokens):\n",
    "    tags = defaultdict(lambda : wn.NOUN)\n",
    "    tags['J'] = wn.ADJ\n",
    "    tags['V'] = wn.VERB\n",
    "    tags['R'] = wn.ADV\n",
    "\n",
    "    lemmitizer = WordNetLemmatizer()\n",
    "    new_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemmitizer.lemmatize(token, tags[tag[0]])\n",
    "        new_tokens.append(lemma)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: lemmatize_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top 1000 words used in the reviews\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Frequency Distribution:\n",
    "freq_dist = nltk.FreqDist(word_tokenize(newText))\n",
    "# top 1000 frequentwords\n",
    "print(freq_dist.most_common(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = ['chicken','sauce','drink','cheese','burger','salad','meat','rice','dessert','pizza','beef','steak','sushi','bread',\n",
    "       'soup','pork','egg','shrimp','sandwich','potato','buffet','beer','cake','appetizer','chocolate','ramen','green',\n",
    "       'coffee','crab','onion','taco','garlic','thai','bacon','rib','wine','chip','bbq','tomato','bean','salmon',\n",
    "       'mushroom','butter','lobster','seafood','corn','pepper','pancake','pasta','veggie','tuna','korean','mac','n','cocktail',\n",
    "       'sausage','waffle','salt','asian','oyster','salsa','ingredient','eye','truffle','lemon','pie','chinese','vegan',\n",
    "       'strawberry','shake','chili','tofu','duck','avocado','banana','lettuce','vegetable','calamari','bake','spinach','crepe',\n",
    "       'pastry','filet','scallop','bone','fruit','meatball','juice','tempura','cheesecake','apple','mango','orange','coconut',\n",
    "       'pickle','gravy','hawaiian','brisket','caesar','vegetarian','tortilla','sashimi','pudding','margarita','cucumber',\n",
    "       'vanilla','tacos','basil','patty','sprout','mayo','soda','guacamole','peanut','latte','lime','syrup','paemesan','vietnamese',\n",
    "       'ranch','ribeye','octopus','ginger','pineapple','cinnamon','katsu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['chicken','drink','cheese','burger','pizza','coffee','chocolate','wine','veggie','vegetarian','cake','rice','meat',\n",
    "           'steak','bread','pork','appetizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 2 vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'review': df['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases take list of list as input\n",
    "reviews = [row.split() for row in df1['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(reviews, min_count=30, progress_per=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec model implementation\n",
    "# Parameters to word2vec model\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "w2v_model.build_vocab(sentences, progress_per=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the word2vec Model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features from the features selected from top 1000 words\n",
    "featuresGenerated = []\n",
    "i = 0\n",
    "for j in features:\n",
    "    # similar words generation\n",
    "    x = w2v_model.wv.most_similar(positive=[j])\n",
    "    y = []\n",
    "    i +=1\n",
    "    y.append(i)\n",
    "    y.append(j)\n",
    "    for k in x:\n",
    "        t = k[0]\n",
    "        y.append(t) \n",
    "    featuresGenerated.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.extend(['beverage','lemonade','mimosa','beverage','manchego','bleu','hamburger','cheeseburger','pepperoni',\n",
    "                    'espresso','cappucino','caramel','marshmallow','buterscotch','toffee','riesling','champagne','cabernet',\n",
    "                   'mushroom','buttercream','mousse','fillet','sourdough','baguette','garlic','chasu'])\n",
    "len(food)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
